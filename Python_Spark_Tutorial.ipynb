{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PySpark Tutorial\n",
    "================\n",
    "\n",
    "_Djoerd Hiemstra and Robin Aly, University of Twente_\n",
    "\n",
    "In this tutorial (made for the SIKS/CBS DataCamp, 6 December 2016) we will go over some basic Spark Scala examples following the paper by ([Zahari et al. 2012][1]). You can edit this tutorial by double-clicking its cells. You can run a cell by pressing _shift-enter_.\n",
    "\n",
    "[1]: https://amplab.cs.berkeley.edu/wp-content/uploads/2012/01/nsdi_spark.pdf \"Matei Zaharia, Mosharaf Chowdhury, Tathagata Das, Ankur Dave, Justin Ma, Murphy McCauley, Michael Franklin, Scott Shenker, Ion Stoica. Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing. Proceedings of the USENIX Symposium of Networked Systems Design and Implementation, 2012.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Spark using RDDs\n",
    "\n",
    "Suppose we would like to analyse a huge log file, for instance a search engine's query log. The following line reads the contents of the file `log.txt` into a RDD (Resilient Distributed Dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# necessary prolog\n",
    "from pyspark import SparkContext\n",
    "try:\n",
    "    sc\n",
    "except NameError:\n",
    "    sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lines = sc.textFile(\"log.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NB We called the variable `spark` in the paper by Zahari et al. (2012) `sc` in our case (for 'Spark context').\n",
    ">\n",
    "> Note that the paper uses Scala examples, not Python.\n",
    "\n",
    "An RDD is a read-only collection of records that may be partitioned over many machines in your cluster. RDDs can only be created from other RDDs through a limited number of operations called _transformations_ (they may also be read from the distributed file system). Table 2 of Zahari et al. (2012) contains 13 transformations of RDDs. Why these transformations? Together these transformations support many algorithms, and, the transformations can be executed efficiently in parallel on clusters of machines. \n",
    "\n",
    "As an example of a transformation, `map()` takes as input a function, and applies this function to each record in the RDD. The functions that may be provided to map may not have any _side effects_, that is, they may input a record of the RDD and output a transformed record, but they cannot read or write files, nor can they have an internal state that is updated. If functions without side effects are used, then each machine in a large cluster can perform the function on part of the data without needing to know anything about results from the other machines on other parts of the data. As an example, take the following function that takes a line and outputs the line with all letters put to lower case:\n",
    "\n",
    "    lambda line: line.lower()\n",
    "    \n",
    "This is called an _lambda_ function in Python. Lambda functions are anonymous functions because they have no name. The paper uses Scala syntax for anonymous functions, i.e. `line => line.toLowerCase()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lowerLines = lines.map(lambda line: line.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anonymous functions are a consice way to use a function once (maybe they should have been called _disposable functions_). The same line with a named function would be:\n",
    "\n",
    "\n",
    "    def toLower(line):\n",
    "        return line.lower()\n",
    "    \n",
    "    lowerLines2 = lines.map(toLower)\n",
    "\n",
    "Spark runs all your operations on RDDs in parallel. If you want to do something in linear order in plain old Python, for instance outputting the contents of the RDD, then the function `collect()` turns your RDD into an ordinary Python array.\n",
    "\n",
    "> NB Beware, the Jupyter notebook might only show part of your result.\n",
    "\n",
    "> The `\\t` strings in the output denote tabs in the original file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lines.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.1, count the number of errors in the log\n",
    "\n",
    "Follow the examples from Zaharia et al. (2012), and print the number of lines that start with \"ERROR\". Your solution should print: 4.\n",
    "\n",
    "\n",
    "> Comments in Python are preceded by a hash `'#'`\n",
    "\n",
    "> To divide a long statement over multiple lines in Python, end a line with a backslah `'\\'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION \n",
    "errors = lines.filter(lambda line: line.startswith(\"ERROR\")).persist()\n",
    "errors.count()\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Exercise 1.2, count errors mentioning chimney\n",
    "\n",
    "Follow the examples from Zaharia et al. (2012), and print the number of lines that start with \"ERROR\" and that contain \"chimney\". Your solution should print: 3. Tip: you might build on the result of the previous question.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION \n",
    "\n",
    "errors \\\n",
    "    .filter(lambda line: line.find(\"chimney\") != -1) \\\n",
    "    .count()\n",
    "\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Exercise 1.3, return the time fields of the errors mentioning 'chimney'\n",
    "\n",
    "Follow the examples from Zaharia et al. (2012), and print the time fields of the lines that start with \"ERROR\" and that contain \"chimney\". Your solution should print: `2016-12-05 18:00:08, 2016-12-05 18:01:06,` and `2016-12-05 18:01:06`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION \n",
    "\n",
    "errors \\\n",
    "    .filter(lambda line: line.find(\"chimney\") != -1) \\\n",
    "    .map(lambda line: line.split(\"\\t\")[1]) \\\n",
    "    .collect()\n",
    "    \n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.4, the seminal MapReduce word count\n",
    "\n",
    "In 2004, Google employees Jeff Dean and Sanjay Ghemawat proposed a framework for distributed data processing that supports only 2 transformations: `map()` and `reduce()`. They called their framework appropriately MapReduce ([Dean and Ghemawat 2004][1]). \n",
    "\n",
    "The seminal example they introduce in their paper is _word count_: Input a large text corpus, and output all words with for each word its count, i.e. the total number of times it occurs in the text corpus. A naive implementation might update a global data structure for each word that it encounters, adding 1 for the particular word. However, remember that we need functions *without* side effects to be able to distribute computations over many machines (so no updating a data structure!). Dean and Ghemawat therefore propose a solution that splits words and outputs pairs (_word_, 1) in the \"map phase\"; and then adds the 1's in the \"reduce phase\" (after the framework groups all data with the same together). Spark can process every MapReduce algorithm (and many more complex algorithms) using its transformations. Study the word count solution by Dean and Ghemawat, and come up with the equivalent Spark solution. See also the remarks by Zahari et al. (2012), for instance in Section 7.1. \n",
    "\n",
    "Execute _word count_ on the message fields of the lines that start with \"ERROR\". Your solution should find that the three most occurring words are \"error\", \"descend\" and \"chimney\", which occur respectively 5, 3 and 3 times.\n",
    "\n",
    "> Tip: build your solution one transformation at a time: Start from the lines that start with \"ERROR\", then take the message field, then split the (lower-cased) fields on space \" \" to get the words, then transform each word to (word, 1), etc. Test your solution after adding each transformation.\n",
    "\n",
    "[1]: http://research.google.com/archive/mapreduce-osdi04.pdf \"Jeffrey Dean and Sanjay Ghemawat. MapReduce: Simplified Data Processing on Large Clusters, In Proceedings of the 6th Symposium on Operating System Design and Implementation (OSDI), 2004\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION \n",
    "\n",
    "errors \\\n",
    "    .map(lambda line: line.split(\"\\t\")[3]) \\\n",
    "    .flatMap(lambda line:line.lower().split(\" \")) \\\n",
    "    .map(lambda word: (word, 1)) \\\n",
    "    .reduceByKey(lambda a, b: a + b) \\\n",
    "    .sortBy(lambda x: -x[1]) \\\n",
    "    .collect()\n",
    "\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further exercises\n",
    "\n",
    "Zahari et al. (2012) cover two more elegant examples of iterative algorithms that can be efficiently processed by Spark: _logistic regression_, and Google's _PageRank_. Similar solutions exist using MapReduce, but they are less efficient, because MapReduce writes all intermediate data to disk for each iteration of the algorithm, whereas Spark tries to keep data in memory, if possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Spark using DataFrames\n",
    "\n",
    "In the following examples we use Spark with Dataframes. In Spark, a DataFrame is a distributed collection of data organized into _named_ columns. It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with the optimizations provided by RDDs under the hood. We base this part of the tutorial on the SIGMOD 2015 paper by ([Armbrust et al. 2015][2])\n",
    "\n",
    "Let's read some example data in a DataFrame using the standard environment variable `sqlContext`. We read data from a json file, because it is _self-describing_, that is, it contains the schema information needed for DataFrames (except for the table name).\n",
    "\n",
    "[2]: https://amplab.cs.berkeley.edu/wp-content/uploads/2015/03/SparkSQLSigmod2015.pdf \"Michael Armbrust, Reynold S. Xin, Cheng Lian, Yin Huai, Davies Liu, Joseph K. Bradley, Xiangrui Meng, Tomer Kaftan, Michael J. Franklin, Ali Ghodsi, Matei Zaharia. In. Proceedings of the International Conference on Management of Data (SIGMOD), 2015\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# neceesary prolog for dataframes\n",
    "from pyspark import sql\n",
    "sqlContext = sql.SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- gender: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- organization: string (nullable = true)\n",
      "\n",
      "+------+---------+------------+\n",
      "|gender|     name|organization|\n",
      "+------+---------+------------+\n",
      "|  male|    Arjen|          RU|\n",
      "|  male|   Djoerd|          UT|\n",
      "|  male|    Robin|          UT|\n",
      "|  male|     Yuri|          UT|\n",
      "|female|    Doina|          UT|\n",
      "|female|     Anna|          UT|\n",
      "|  male|     Piet|         CBS|\n",
      "|  male|  Barteld|         CBS|\n",
      "|female|Jacobiene|         CBS|\n",
      "|  male|    Marco|         CBS|\n",
      "|female|  Claudia|         TUD|\n",
      "+------+---------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people = sqlContext.read.json(\"people.json\") \n",
    "\n",
    "people.printSchema()\n",
    "people.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrames support special functions like `printSchema()` and `show()` and the common [relational algebra operations](https://en.wikipedia.org/wiki/Relational_algebra): projection, called `select()`; selection, called `where()`; and join, called `join()`, as well as aggregations: (`groupBy()` and `agg()`).\n",
    "\n",
    "> Yes, you are right: Someone really messed up naming the relational algebra operations! The naming of relational algebra operations differs in some unfortunate ways from the naming used in SQL statements. The Spark algebra operations use the SQL conventions, to make the confusion complete. \n",
    "\n",
    "So, like RDDs, DataFrames support a limited number of transformations (but now based on relational algebra), and that's what we will have to work with. However, we might also work directly in SQL (see Exercise 2.4 below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.1, select all names\n",
    "\n",
    "Show the list of names of all people (the projection of the column 'name').\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|     name|\n",
      "+---------+\n",
      "|    Arjen|\n",
      "|   Djoerd|\n",
      "|    Robin|\n",
      "|     Yuri|\n",
      "|    Doina|\n",
      "|     Anna|\n",
      "|     Piet|\n",
      "|  Barteld|\n",
      "|Jacobiene|\n",
      "|    Marco|\n",
      "|  Claudia|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# START SOLUTION\n",
    "people.select(\"name\").show()\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2, select all names of people from CBS\n",
    "\n",
    "Show the list of names of people that work at CBS (Can we first do the projection of the column 'name' and then the selection of the row for which `organization=\"CBS\"`?).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|     name|\n",
      "+---------+\n",
      "|     Piet|\n",
      "|  Barteld|\n",
      "|Jacobiene|\n",
      "|    Marco|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# START SOLUTION\n",
    "people.where(people[\"organization\"] == \"CBS\").select(\"name\").show()\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "Let's introduce the following organization DataFrame. Interestingly, this DataFrame contains two structured columns, a feature known from object-relational databases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- address: struct (nullable = true)\n",
      " |    |-- city: string (nullable = true)\n",
      " |    |-- number: long (nullable = true)\n",
      " |    |-- street: string (nullable = true)\n",
      " |-- attractions: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- color: string (nullable = true)\n",
      " |-- organization: string (nullable = true)\n",
      "\n",
      "+--------------------+--------------------+-----+------------+\n",
      "|             address|         attractions|color|organization|\n",
      "+--------------------+--------------------+-----+------------+\n",
      "|[Enschede,5,drien...|[torentje, hadoop...|black|          UT|\n",
      "|[Heerlen,11,CBS-weg]|[mijnmuseum, spar...| blue|         CBS|\n",
      "|[Nijmegen,4,Comen...|       [doornroosje]|  red|          RU|\n",
      "|   [Delft,5,Postbus]|      [nice cluster]| blue|         TUD|\n",
      "+--------------------+--------------------+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "organizations = sqlContext.read.json(\"organizations.json\") \n",
    "organizations.printSchema()\n",
    "organizations.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.3, people in blue organizations\n",
    "\n",
    "Show the name and organization of people whose organization's color is blue by joining the two tables. \n",
    "\n",
    "> Question: Can we speed up the computation by changing the order of the statements?\n",
    "\n",
    "> Bonus: Count for each organization the number of people, outputting (\"organization\", \"count\"). Note that the Armbrust et al. paper contains an error in Section 3.3: the operation `.agg(count(\"name\"))` should be `.count()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+\n",
      "|     name|organization|\n",
      "+---------+------------+\n",
      "|     Piet|         CBS|\n",
      "|  Barteld|         CBS|\n",
      "|Jacobiene|         CBS|\n",
      "|    Marco|         CBS|\n",
      "|  Claudia|         TUD|\n",
      "+---------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# START SOLUTION\n",
    "people\\\n",
    "    .join(organizations, people[\"organization\"] == organizations[\"organization\"])\\\n",
    "    .where(organizations[\"color\"] == \"blue\")\\\n",
    "    .select(people[\"name\"], organizations[\"organization\"])\\\n",
    "    .show();\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "If you are familiar with SQL, then the following approach would also find the people with blue organizations. Once we registered the DataFrames `people` and `organizations`, we can use `sqlContext.sql()` to execute any SQL query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+\n",
      "|     name|organization|\n",
      "+---------+------------+\n",
      "|     Piet|         CBS|\n",
      "|  Barteld|         CBS|\n",
      "|Jacobiene|         CBS|\n",
      "|    Marco|         CBS|\n",
      "|  Claudia|         TUD|\n",
      "+---------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.registerTempTable(\"people\");\n",
    "organizations.registerTempTable(\"organizations\");\n",
    "sqlContext.sql('''\n",
    "SELECT P.name, O.organization\n",
    "FROM people P, organizations O\n",
    "WHERE P.organization = O.organization\n",
    "AND O.color = 'blue'\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.4, women from Enschede\n",
    "\n",
    "In SQL, the complex types `address` and `attractions` are available as follows: For instance, `Organizations.address.city = 'Heerlen'` for organizations in Heerlen. \n",
    "Count the number of female employees in Enschede. Your answer should be: 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|enschedeWomen|\n",
      "+-------------+\n",
      "|            2|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# START SOLUTION\n",
    "sqlContext.sql('''\n",
    "SELECT count(name) AS enschedeWomen\n",
    "FROM people P, organizations O\n",
    "WHERE P.organization = O.organization\n",
    "AND P.gender = 'female'\n",
    "AND O.address.city = 'Enschede'\n",
    "'''\n",
    ").show()\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further exercises\n",
    "\n",
    "New to SQL? Then we recommend additional SQL exercises, for instance [Learn SQL from Codecademy](https://www.codecademy.com/learn/learn-sql)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  DataCamp sample data\n",
    "\n",
    "Below you find a samples of the data that is available on the DataCamp Hadoop/Spark cluster. Use the sample data to develop and test your Spark scripts before executing them on the cluster.\n",
    "\n",
    "\n",
    "### Dutch Tweets\n",
    "We use the Twitter data described by ([Tjong-Kim-Sang and Van den Bosch 2013][3]) which is available on the Twente Hadoop cluster under: `/data/twitterNL`. \n",
    "\n",
    "[3]: http://ifarm.nl/erikt/papers/clin2013.pdf \"Erik Tjong Kim Sang and Antal van den Bosch. Dealing with big data: The case of Twitter. In: Computational Linguistics in the Netherlands Journal 3, ISSN: 2211-4009, pages 121-134, 2013.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                text|\n",
      "+--------------------+\n",
      "|Pinnen over je wa...|\n",
      "|@AllesofNooit @He...|\n",
      "|#dooba #sexdate J...|\n",
      "|En weer een uur v...|\n",
      "|Frederik Meulewae...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets = sqlContext.read.json(\"tweets.json.gz\") \n",
    "\n",
    "# uncomment to print the (crazy) schema:\n",
    "# tweets.printSchema()\n",
    "\n",
    "tweets.select(\"text\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### AIS data\n",
    "\n",
    "The [Automatic identification system](https://en.wikipedia.org/wiki/Automatic_identification_system) (AIS) is an automatic tracking system used on ships and by vessel traffic services (VTS) for identifying and locating vessels by electronically exchanging data with other nearby ships, AIS base stations, and satellites. The data is available on the Twente Hadoop cluster under: `/data/aisUT`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- callsign: string (nullable = true)\n",
      " |-- cog: long (nullable = true)\n",
      " |-- destination: string (nullable = true)\n",
      " |-- dimbow: long (nullable = true)\n",
      " |-- dimport: long (nullable = true)\n",
      " |-- dimstarboard: long (nullable = true)\n",
      " |-- dimstern: long (nullable = true)\n",
      " |-- draught: long (nullable = true)\n",
      " |-- eta_day: long (nullable = true)\n",
      " |-- eta_hour: long (nullable = true)\n",
      " |-- eta_minute: long (nullable = true)\n",
      " |-- eta_month: long (nullable = true)\n",
      " |-- heading: long (nullable = true)\n",
      " |-- imo: long (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- lat2: string (nullable = true)\n",
      " |-- lon: double (nullable = true)\n",
      " |-- lon2: string (nullable = true)\n",
      " |-- mmsi: long (nullable = true)\n",
      " |-- nav_status: long (nullable = true)\n",
      " |-- rot_angle: double (nullable = true)\n",
      " |-- rot_direction: string (nullable = true)\n",
      " |-- shipname: string (nullable = true)\n",
      " |-- shiptype: long (nullable = true)\n",
      " |-- sog: long (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- type: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ais = sqlContext.read.json(\"ais.json.gz\") \n",
    "# ais.show(5)\n",
    "ais.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDW data\n",
    "The Rijksdienst Wegverkeer is the Dutch ministery the takes care of the public roads. The data contains measurements from sensors in the roads. This data is available as comma-separated value (CSV) files and is available on the Twente Hadoop cluster under: `/data/cbs/loopraw`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['RWS01_MONIBAS_0091hrl0763ra', '1', '11B'],\n",
       " ['RWS01_MONIBAS_0091hrl0763ra', '1', '12B'],\n",
       " ['RWS01_MONIBAS_0091hrl0785ra', '1', '1B'],\n",
       " ['RWS01_MONIBAS_0091hrl0785ra', '1', '2B'],\n",
       " ['RWS01_MONIBAS_0091hrl0785ra', '1', '3B'],\n",
       " ['RWS01_MONIBAS_0091hrl0785ra', '1', '4B'],\n",
       " ['RWS01_MONIBAS_0091hrl0785ra', '1', '9B'],\n",
       " ['RWS01_MONIBAS_0091hrl0785ra', '1', '10B'],\n",
       " ['RWS01_MONIBAS_0091hrl0785ra', '1', '11B'],\n",
       " ['RWS01_MONIBAS_0091hrl0785ra', '1', '12B'],\n",
       " ['RWS01_MONIBAS_0091hrl0794ra', '1', '1B'],\n",
       " ['RWS01_MONIBAS_0091hrl0794ra', '1', '2B'],\n",
       " ['RWS01_MONIBAS_0091hrl0794ra', '1', '3B'],\n",
       " ['RWS01_MONIBAS_0091hrl0794ra', '1', '4B'],\n",
       " ['RWS01_MONIBAS_0091hrl0794ra', '1', '9B'],\n",
       " ['RWS01_MONIBAS_0091hrl0794ra', '1', '10B'],\n",
       " ['RWS01_MONIBAS_0091hrl0794ra', '1', '11B'],\n",
       " ['RWS01_MONIBAS_0091hrl0794ra', '1', '12B'],\n",
       " ['RWS01_MONIBAS_0091hrr0059ra', '1', '1A'],\n",
       " ['RWS01_MONIBAS_0091hrr0059ra', '1', '3A']]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdw = sc.textFile(\"rdw.csv.gz\")\n",
    "rdw.map(lambda line: line.split(',')[:3]).collect()[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Volkskrant data\n",
    "The Dutch netwspaker [Volkskrant](http://www.volkskrant.nl) has a large archive of its articles online. We downloaded the years 2000 - 2016 and they are available on the Twente Hadoop cluster under: `/data/volkskrant`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+--------------------+-----+--------------------+--------------------+---------+--------------------+----+\n",
      "|category|day|                href|month|                text|                time|timeofday|               title|year|\n",
      "+--------+---+--------------------+-----+--------------------+--------------------+---------+--------------------+----+\n",
      "| Archief|  5|http://www.volksk...|    7|Een provocatieve ...|  5 juli 2011, 00:00|    00:00|Mensen uitlachen ...|2011|\n",
      "|Politiek| 18|http://www.volksk...|    3|Een euforisch gej...|18 maart 2015, 22:39|    22:39|'De boodschap van...|2015|\n",
      "|Economie| 21|http://www.volksk...|    6|De klant is konin...| 21 juni 2008, 02:47|    02:47|Klant geen koning...|2008|\n",
      "|Magazine|  4|http://www.volksk...|    1|Het gaat van kwaa...|4 januari 2013, 1...|    12:04|Indiase politicus...|2013|\n",
      "| Archief| 29|http://www.volksk...|    3|Chris Klomp vindt...|29 maart 2013, 00:00|    00:00|Uitgedaagd op Vol...|2013|\n",
      "+--------+---+--------------------+-----+--------------------+--------------------+---------+--------------------+----+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- category: string (nullable = true)\n",
      " |-- day: long (nullable = true)\n",
      " |-- href: string (nullable = true)\n",
      " |-- month: long (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      " |-- timeofday: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "volkskrant = sqlContext.read.json(\"volkskrant.json.gz\") \n",
    "\n",
    "volkskrant.show(5)\n",
    "\n",
    "volkskrant.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
